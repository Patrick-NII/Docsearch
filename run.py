#!/usr/bin/env python3
"""
DocSearch - Assistant IA de Lecture de Document
Point d'entr√©e de l'application
"""

import os
import sys
from pathlib import Path

# Ajouter le r√©pertoire courant au path
sys.path.append(str(Path(__file__).parent))

from config import Config
from document_loader import DocumentLoader
from text_splitter import TextSplitter
from vector_store import VectorStore
from rag_chain import RAGChain
from qa_interface import QAInterface
import streamlit as st
import logging

logger = logging.getLogger(__name__)

def check_dependencies():
    """V√©rifie que toutes les d√©pendances sont install√©es"""
    print("üîç V√©rification des d√©pendances...")
    
    try:
        import streamlit
        import langchain
        import chromadb
        import PyPDF2
        import sentence_transformers
        print("‚úÖ D√©pendances OK")
        return True
    except ImportError as e:
        print(f"‚ùå D√©pendance manquante: {e}")
        return False

def check_ollama():
    """V√©rifie qu'Ollama est disponible"""
    print("üîë V√©rification d'Ollama...")
    
    try:
        from langchain_community.llms import Ollama
        # Test simple de connexion
        llm = Ollama(model="llama2")
        print("‚úÖ Ollama disponible")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Ollama non disponible: {e}")
        print("üí° Installez Ollama et t√©l√©chargez un mod√®le avec: ollama pull llama2")
        return False

def setup_directories():
    """Cr√©e les r√©pertoires n√©cessaires"""
    Path(Config.SOURCE_DIR).mkdir(exist_ok=True)
    Path(Config.VECTOR_DB_PATH).mkdir(exist_ok=True)

def load_documents():
    """Charge et indexe les documents"""
    print("üìÑ Chargement des documents...")
    
    # Charger les documents
    loader = DocumentLoader()
    documents = loader.load_documents(Config.SOURCE_DIR)
    
    if not documents:
        print("‚ö†Ô∏è Aucun document trouv√© dans le dossier 'source/'")
        print("üí° Placez vos fichiers PDF ou TXT dans le dossier 'source/'")
        return None, None
    
    print(f"üìÑ {len(documents)} document(s) trouv√©(s)")
    
    # D√©couper en chunks
    splitter = TextSplitter(
        chunk_size=Config.CHUNK_SIZE,
        chunk_overlap=Config.CHUNK_OVERLAP
    )
    chunks = splitter.split_documents(documents)
    
    # Indexer dans la base vectorielle
    vector_store = VectorStore(Config.VECTOR_DB_PATH)
    vector_store.add_chunks(chunks)
    
    return vector_store, chunks

def main():
    """Fonction principale"""
    print("üöÄ Assistant IA de Lecture de Document")
    print("=" * 50)
    
    # V√©rifications pr√©liminaires
    if not check_dependencies():
        return
    
    # Charger la configuration
    Config.load_config()
    
    # V√©rifier Ollama
    if not check_ollama():
        print("‚ö†Ô∏è L'application peut fonctionner sans Ollama, mais certaines fonctionnalit√©s seront limit√©es")
    
    # Cr√©er les r√©pertoires
    setup_directories()
    
    # Charger les documents
    vector_store, chunks = load_documents()
    
    if vector_store is None:
        print("‚ùå Impossible de charger les documents")
        return
    
    print("\nüéØ Lancement de l'application...")
    print("üåê L'application sera accessible √† l'adresse: http://localhost:8501")
    print("üõë Appuyez sur Ctrl+C pour arr√™ter l'application")
    print("=" * 50)
    
    # Cr√©er la cha√Æne RAG
    rag_chain = RAGChain(vector_store)
    
    # Cr√©er l'interface
    interface = QAInterface(rag_chain)
    
    # Lancer l'interface Streamlit
    interface.run()

if __name__ == "__main__":
    main() 